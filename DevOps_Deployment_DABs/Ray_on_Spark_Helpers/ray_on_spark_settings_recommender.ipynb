{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee937a39-804a-43d2-8d88-7e4c11359f37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ray on Spark - Cluster Setup Recommender\n",
    "\n",
    "### Overview\n",
    "* This script will provide recommendations for configurations for the `ray.util.spark.setup_ray_cluster()` command to launch a \"Ray on Spark\" cluster. \n",
    "* Attach it to any Classic All-Purpose Cluster during development; you can then take the recommendations after testing to an automated Job Cluster. \n",
    "* *As of August 2025, Ray on Spark will not work on Serverless clusters*\n",
    "* The setup script will also confirm baseline cluster settings (such as runtime version, security mode).\n",
    "\n",
    "### Steps\n",
    "* Attach this script to a Classic All-Purpose cluster\n",
    "* Click \"Run all\"\n",
    "* Copy/Paste the [Ray on Spark setup command](https://docs.databricks.com/aws/en/machine-learning/ray/ray-create) at the end into a different notebook, then run (on the same cluster).\n",
    "* Continue to modify this baseline setup as your workload evolves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23f543e3-f900-4f83-9b24-1f262ad32990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qqq --upgrade databricks-sdk \n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b6fee43-1306-457f-bbf3-8c2e9499dcf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "def get_workspace_client() -> WorkspaceClient: \n",
    "    \"\"\"\n",
    "    Returns an authenticated WorkspaceClient using the current Databricks notebook context.\n",
    "    \"\"\"\n",
    "    ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "    DATABRICKS_TOKEN = ctx.apiToken().getOrElse(None)\n",
    "    DATABRICKS_URL = ctx.apiUrl().getOrElse(None)\n",
    "    return WorkspaceClient(host=DATABRICKS_URL, token=DATABRICKS_TOKEN)\n",
    "  \n",
    "def get_cluster_id() -> str:\n",
    "    \"\"\"\n",
    "    Returns the cluster ID of the current Databricks notebook context.\n",
    "    \"\"\"\n",
    "    return dbutils.notebook.entry_point.getDbutils().notebook().getContext().clusterId().get()\n",
    "  \n",
    "w = get_workspace_client()\n",
    "\n",
    "current_cluster = w.clusters.get(get_cluster_id())\n",
    "# pprint(current_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef9fa50b-a738-4486-8577-e7775fa529f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_security_mode(current_cluster):\n",
    "    \"\"\"\n",
    "    Checks if the current cluster's data_security_mode is allowed for running Ray on Spark.\n",
    "    Raises a ValueError if the mode is not permitted.\n",
    "    \"\"\"\n",
    "    allowed_modes = [\"USER_ISOLATION\", \"SINGLE_USER\", \"DEDICATED\", \"NONE\"]\n",
    "    pattern = re.compile(rf\"({'|'.join(allowed_modes)})\")\n",
    "\n",
    "    data_security_mode_str = str(current_cluster.data_security_mode)\n",
    "\n",
    "    if not pattern.search(data_security_mode_str):\n",
    "        raise ValueError(\n",
    "            f\"data_security_mode '{data_security_mode_str}' is not allowed for Ray on Spark clusters. Allowed: {allowed_modes}. See docs: https://docs.databricks.com/aws/en/machine-learning/ray/#limitations\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Clusters with data_security_mode '{data_security_mode_str}' can be used to create Ray on Spark clusters.\")\n",
    "\n",
    "check_security_mode(current_cluster=current_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f4f4b1b-82de-46aa-b586-ba1f01ab5fc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_runtime_version(current_cluster):\n",
    "    \"\"\"\n",
    "    Checks if the current cluster's spark_version meets the minimum required version for running Ray on Spark.\n",
    "    Raises a ValueError if the version is not permitted. Also recommends using the ML Runtime for best compatibility.\n",
    "    \"\"\"\n",
    "    allowed_starting_version = [\"12.2\"]\n",
    "    # Extract numeric parts from spark_version (e.g., \"12.2.x-cpu-ml-scala2.12\" -> \"12.2\")\n",
    "    version_match = re.match(r\"(\\d+\\.\\d+)\", str(current_cluster.spark_version))\n",
    "    if not version_match:\n",
    "        raise ValueError(f\"Could not parse spark_version: {current_cluster.spark_version}\")\n",
    "    spark_version_num = version_match.group(1)\n",
    "\n",
    "    # Compare as floats\n",
    "    if float(spark_version_num) < float(allowed_starting_version[0]):\n",
    "        raise ValueError(\n",
    "            f\"Cluster spark_version '{current_cluster.spark_version}' is less than the required version {allowed_starting_version[0]}\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Cluster Runtime '{current_cluster.spark_version}' can be used to run Ray on Spark. Consider upgrading to latest stable LTS ML Runtime for the best performance. \")\n",
    "    \n",
    "    if not getattr(current_cluster, \"use_ml_runtime\", False):\n",
    "        print(\"Your cluster is not using ML Runtime. Recommend upgrading to most recent LTS Machine Learning Runtime where Ray and other dependencies are pre-installed\")\n",
    "    \n",
    "check_runtime_version(current_cluster=current_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7307455-4504-4242-b800-e61ea55ca4a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_single_node(current_cluster):\n",
    "  \"\"\"\n",
    "  Checks if the current cluster is a single-node cluster.\n",
    "  Raises a ValueError if the cluster is single-node, as Ray on Spark setup is intended for multi-node clusters.\n",
    "  \"\"\"\n",
    "  if getattr(current_cluster, \"is_single_node\", False):\n",
    "    raise ValueError(\"This script is intended to determine setup for a multi-node cluster to use Ray on Spark. This is a single-node cluster. To use ray, just run ray.init(). See Ray docs for more info.\")\n",
    "\n",
    "check_single_node(current_cluster=current_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8588f1ae-bbbc-414d-a211-e9937edddd3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_gpu(current_cluster):\n",
    "    \"\"\"\n",
    "    Checks if the current cluster is using GPU-enabled instance types based on the cloud provider and node type.\n",
    "    Returns True if GPUs are detected, otherwise False.\n",
    "    \"\"\"\n",
    "    gpus = {\n",
    "        \"gpu_instance_types\": {\n",
    "            \"aws\": [\"p5\", \"p4\", \"g6e\", \"g6\", \"g5\", \"g4dn\", \"p3\"],\n",
    "            \"azure\": [\n",
    "                \"Standard_NC40ads_H100_v5\",\n",
    "                \"Standard_NC80adis_H100_v5\",\n",
    "                \"Standard_NC24ads_A100_v4\",\n",
    "                \"Standard_NC48ads_A100_v4\",\n",
    "                \"Standard_NC96ads_A100_v4\",\n",
    "                \"Standard_ND96asr_v4\",\n",
    "                \"Standard_NV36ads_A10_v5\",\n",
    "                \"Standard_NV36adms_A10_v5\",\n",
    "                \"Standard_NV72ads_A10_v5\",\n",
    "                \"Standard_NC4as_T4_v3\",\n",
    "                \"Standard_NC8as_T4_v3\",\n",
    "                \"Standard_NC16as_T4_v3\",\n",
    "                \"Standard_NC64as_T4_v3\",\n",
    "                \"Standard_NC6s_v3\",\n",
    "                \"Standard_NC12s_v3\",\n",
    "                \"Standard_NC24s_v3\",\n",
    "                \"Standard_NC24rs_v3\",\n",
    "            ],\n",
    "            \"gcp\": [\n",
    "                \"a2-ultragpu-8g\",\n",
    "                \"a2-highgpu-1g\",\n",
    "                \"a2-highgpu-2g\",\n",
    "                \"a2-highgpu-4g\",\n",
    "                \"a2-megagpu-16g\",\n",
    "                \"g2-standard-8\",\n",
    "            ],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if hasattr(current_cluster, \"aws_attributes\"):\n",
    "        current_cloud = \"aws\"\n",
    "    elif hasattr(current_cluster, \"azure_attributes\"):\n",
    "        current_cloud = \"azure\"\n",
    "    elif hasattr(current_cluster, \"gcp_attributes\"):\n",
    "        current_cloud = \"gcp\"\n",
    "    else:\n",
    "        current_cloud = None\n",
    "\n",
    "    has_gpu = False\n",
    "    if current_cloud and hasattr(current_cluster, \"node_type_id\"):\n",
    "        node_type = str(current_cluster.node_type_id).lower()\n",
    "        gpu_types = [t.lower() for t in gpus[\"gpu_instance_types\"].get(current_cloud, [])]\n",
    "        for gpu_type in gpu_types:\n",
    "            if gpu_type in node_type:\n",
    "                has_gpu = True\n",
    "                # print(\"Using instances with GPUs, additional setup required.\")\n",
    "                break\n",
    "\n",
    "    # else:\n",
    "      # print(\"CPU-only instances.\")\n",
    "    return has_gpu\n",
    "\n",
    "check_gpu(current_cluster=current_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ee9dd96-c856-449d-acec-973f4d4b14f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Change to function input\n",
    "spark_share = 0.0\n",
    "\n",
    "setup_cmd = \"\"\"\n",
    ">>> Use setup command >>>\n",
    "setup_ray_cluster(\n",
    "\"\"\"\n",
    "\n",
    "print(\"Observations for setup script recommendation:\")\n",
    "# STEP 1: Determine min and max worker nodes\n",
    "## Autoscaling = FALSE\n",
    "if current_cluster.autoscale:\n",
    "  print(\" - Autoscaling cluster\")\n",
    "  min_workers = worker_nodes = current_cluster.autoscale.min_workers\n",
    "  max_workers = current_cluster.autoscale.max_workers\n",
    "\n",
    "  setup_cmd += f\"\"\"  min_worker_nodes={min_workers},\n",
    "  max_worker_nodes={max_workers},\n",
    "  \"\"\"\n",
    "## Autoscaling = TRUE\n",
    "else:\n",
    "  print(\" - Non-Autoscaling cluster\")\n",
    "  worker_nodes = current_cluster.num_workers\n",
    "\n",
    "  setup_cmd += f\"\"\"  min_worker_nodes={worker_nodes},\n",
    "  max_worker_nodes={worker_nodes},\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "# STEP 2: Determine if Driver and Worker nodes match (homogenous cluster)\n",
    "worker_driver_match = current_cluster.driver_node_type_id == current_cluster.node_type_id\n",
    "## Worker Driver Match = FALSE\n",
    "if not worker_driver_match:\n",
    "  print(\" - Heterogenous cluster, Driver and Workers are different instance types\")\n",
    "  driver_cores = int(current_cluster.cluster_cores - spark.sparkContext.defaultParallelism)\n",
    "  worker_cores = int(spark.sparkContext.defaultParallelism / worker_nodes)\n",
    "\n",
    "  setup_cmd += f\"\"\"num_cpus_worker_node={worker_cores},\n",
    "  num_cpus_head_node={driver_cores},\n",
    "  \"\"\"\n",
    "## Homogenouse cluster\n",
    "else:\n",
    "  print(\" - Homogenous cluster, Driver and Workers are same instance type:\")\n",
    "  worker_nodes = current_cluster.num_workers\n",
    "  cores_per_node = int(current_cluster.cluster_cores/(worker_nodes+1))\n",
    "  \n",
    "  setup_cmd += f\"\"\"num_cpus_worker_node={cores_per_node}\n",
    "  num_cpus_head_node={cores_per_node},\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "# STEP 3: Determine if GPUs onboard\n",
    "# TODO: Update and test this\n",
    "if check_gpu(current_cluster):\n",
    "  print(\" - GPU cluster; please manually configure GPUs per worker node\")\n",
    "\n",
    "  setup_cmd += f\"\"\"num_gpus_worker_node=1,\n",
    "  num_gpus_head_node=0,\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "setup_cmd += \"\"\"head_node_options={\n",
    "      'dashboard_port': 9999,\n",
    "      'include_dashboard':True,\n",
    "    }\n",
    ")\n",
    "\"\"\"\n",
    "print(setup_cmd)\n",
    "\n",
    "# STEP 4: Determine if Spark Share is enabled\n",
    "if spark_share > 0.0:\n",
    "  print(\" ^ Determine how many resources to give to Spark, then decrease the values of num_cpus_worker_node and num_cputs_head_node to reserve resources for Spark\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7af88158-adec-44f2-ad92-6f958ed05fa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## End \n",
    "Copy the `setup_ray_cluster()` command printed after running the previous cell."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2818997009354201,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "ray_on_spark_settings_recommender",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
