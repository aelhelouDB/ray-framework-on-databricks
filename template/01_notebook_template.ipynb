{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbc80570-f140-4f30-8e31-7b10c0d16d78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# REPLACE --> Descriptive name for this notebook\n",
    "\n",
    "*REPLACE --> Describe what this notebook demonstrates. Also rename the file using `01_snake_case` naming convention*\n",
    "\n",
    "## Requirements: \n",
    "\n",
    "*REPLACE --> Detail exact cluster configs with which this code runs performantly. Example:*\n",
    "\n",
    "Before proceeding, you need:\n",
    "* A classic compute cluster with the following configurations (see bottom of notebook for example):\n",
    "  * Databricks Runtime: 15.4 LTS ML\n",
    "  * Node Types: 16 core workers (instance type will vary by cloud)\n",
    "  * Number of Workers: 6 (not autoscaling)\n",
    "  * Access Mode: Single-User Dedicated\n",
    "  * Add Spark conf for Spark to Ray conversion ([AWS](https://docs.databricks.com/aws/en/spark/conf)|[Azure](https://learn.microsoft.com/en-us/azure/databricks/spark/conf)|[GCP](https://docs.databricks.com/gcp/en/spark/conf)): `\"spark.databricks.pyspark.dataFrameChunk.enabled\": \"true\"` \n",
    "\n",
    "* Note: As of March 2025, Ray on Databricks does not run on Serverless compute (either notebook or jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c002af3c-5106-4ca8-a0fc-eff1ea2d4d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42952d45-a225-460f-9b26-1380404e1f9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In this section, include any environment setup steps such as users providing inputs (e.g. UC catalog/schema/volume names), pip installing libraries, or setting environment variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e12f59ba-506d-40c1-a7f8-1528e1d2d6f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Solution code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49863b8d-f355-4fc0-b3f5-5b7a8a2584e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Follow numbered header markdown format. Include a \"cleanup\" section at the end to delete created resources, terminate Ray cluster, etc."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_notebook_template",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
